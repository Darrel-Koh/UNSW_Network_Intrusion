{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP model for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline for dat pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Name: Pre-processing Pipeline\n",
    "Author: Khanh Nguyen\n",
    "Description: This file contains the pre-processing pipeline for the dataset that includes label encoding, label processing, and SMOTE.\n",
    "'''\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = 'Binary', smote = False):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "        self.smote = smote\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state', 'attack_cat']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label == 'Binary':\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "        else:             \n",
    "            df['attack_cat'], df['label'] = df['label'], df['attack_cat']\n",
    "            df.drop('attack_cat', axis=1, inplace=True)   \n",
    "\n",
    "        if self.smote:\n",
    "            # Separate features and labels\n",
    "            X = df.drop('label', axis=1)\n",
    "            y = df['label']\n",
    "\n",
    "            # Apply SMOTE for oversampling\n",
    "            smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "            # Convert NumPy arrays back to Pandas DataFrames\n",
    "            X_resampled_df = pd.DataFrame(data=X_resampled, columns=X.columns)\n",
    "            y_resampled_df = pd.DataFrame(data=y_resampled, columns=['label'])\n",
    "\n",
    "            # Concatenate the features and label columns into a single DataFrame\n",
    "            df = pd.concat([X_resampled_df, y_resampled_df], axis=1)   \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_train = PreProcessPipeline(label_encode=True, process_label='Binary')\n",
    "pipeline_test = PreProcessPipeline(label_encode=True, process_label='Binary')\n",
    "train_df = pipeline_train.transform(train_df)\n",
    "test_df = pipeline_test.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the data\n",
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "test_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - Correlation matrix\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = train_df.corr()\n",
    "\n",
    "# Select the correlation values with 'label', label here means attack_cat\n",
    "target_corr = corr_matrix['label']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature ranking in corr order \n",
    "\n",
    "# Calculate the absolute correlation values with the 'label'\n",
    "target_corr_abs = corr_matrix['label'].abs()\n",
    "\n",
    "# Sort the correlation values in descending order\n",
    "sorted_corr = target_corr_abs.sort_values(ascending=False)\n",
    "\n",
    "# Print the sorted correlation values and their corresponding attributes\n",
    "for attribute, correlation in target_corr_abs.items():\n",
    "    print(f\"{attribute}: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316GP2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparktrain_df = spark.createDataFrame(train_df)\n",
    "sparktest_df = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Khanh Nguyen\n",
    "Name: PySpark Dataframe Pipeline\n",
    "Description:\n",
    "    This class is used to create a pipeline for PySpark dataframe, accept 2 boolean parameter: smote & standardize.\n",
    "    Features \n",
    "        (Default)\n",
    "        - Resample: Resample the dataframe\n",
    "        - Vectorize: Vectorize the dataframe\n",
    "        (activate by setting the parameter to True):\n",
    "        - SMOTE: Oversampling the minority class\n",
    "        - Standardize: Standardize the dataframe using z-score\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "class SparkDFPipeline:\n",
    "    def __init__(self, standardize=False):\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, train_df, test_df):      \n",
    "        if self.standardize:\n",
    "            # Standardize the df\n",
    "\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "\n",
    "            exclude = ['proto', 'service', 'state']\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            selected_columns = [col for col in input_columns if col not in exclude]\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "            scaler_model = scaler.fit(train_df)\n",
    "            train_df = scaler_model.transform(train_df)\n",
    "\n",
    "            scaler_model = scaler.fit(test_df)\n",
    "            test_df = scaler_model.transform(test_df)\n",
    "            test_df = test_df.drop('features')\n",
    "            train_df = train_df.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = ['scaled_features', 'proto', 'service', 'state']\n",
    "            output_col = \"features\"\n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "            train_df = assembler1.transform(train_df)\n",
    "            test_df = assembler1.transform(test_df)\n",
    "\n",
    "            # return result\n",
    "            test_df = test_df.select('features', 'label')\n",
    "            train_df = train_df.select('features', 'label')\n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            train_df = train_df.select('features', 'label')\n",
    "            test_df = assembler.transform(test_df)\n",
    "              \n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline example\n",
    "pipeline = SparkDFPipeline(standardize=True)\n",
    "train, test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = len(train.select('features').collect()[0][0])\n",
    "output_length = train.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptronClassifier(seed=42, maxIter=100, stepSize=0.001)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "             .addGrid(mlp.layers, [\n",
    "                    [input_length, 16, 16, 16, output_length],\n",
    "                    [input_length, 32, 25, output_length],\n",
    "             ]) \\\n",
    "             .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=mlp,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3) \n",
    "\n",
    "cv_model = crossval.fit(train)\n",
    "best_model = cv_model.bestModel\n",
    "best_params = best_model.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best parameters for the model\n",
    "param_list = [(param.name, value) for param, value in best_params.items()]\n",
    "shortened_output = \"\\n\".join([f\"{param}: {value}\" for param, value in param_list])\n",
    "print(shortened_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_data, model_name):\n",
    "    # Make predictions on the validation data\n",
    "    predictions = model.transform(val_data)\n",
    "\n",
    "    # REWRITE METRIC CALCULATION\n",
    "\n",
    "    auc_evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate Area Under Precision-Recall Curve using BinaryClassificationEvaluator\n",
    "    pr_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderPR')\n",
    "    area_under_pr = pr_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate True Positives, True Negatives, False Positives, and False Negatives\n",
    "    tp = predictions.filter(\"label = 1 and prediction = 1\").count()\n",
    "    tn = predictions.filter(\"label = 0 and prediction = 0\").count()\n",
    "    fp = predictions.filter(\"label = 0 and prediction = 1\").count()\n",
    "    fn = predictions.filter(\"label = 1 and prediction = 0\").count()\n",
    "\n",
    "    # Calculate metrics using TP, TN, FP, FN\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = sensitivity\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    # youdens_j = sensitivity + specificity - 1\n",
    "    # balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "    # Convert Spark DataFrames to Pandas DataFrames for visualization\n",
    "    y_true_pd = predictions.select('label').toPandas()\n",
    "    y_pred_pd = predictions.select('prediction', 'probability').toPandas()\n",
    "    \n",
    "    # Convert prediction probabilities to binary predictions\n",
    "    y_pred_binary = [1 if prob[1] >= 0.5 else 0 for prob in y_pred_pd['probability']]\n",
    "    cm = confusion_matrix(y_true_pd['label'], y_pred_binary)\n",
    "\n",
    "        # Display the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'Attack'], \n",
    "                    yticklabels=['Normal', 'Attack'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a dictionary of model evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'AUC': auc,\n",
    "        'AreaUnderPR': area_under_pr,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'Accuracy': accuracy,\n",
    "        # \"Youden's J Index\": youdens_j,\n",
    "        # 'Balanced Accuracy': balanced_accuracy\n",
    "    }\n",
    "\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "evaluation_results_default = evaluate_model(best_model, test, 'ANN Model')\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'Default Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_default.keys():\n",
    "    default_value = evaluation_results_default[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
